{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "#import skipthoughts\n",
    "from autocorrect import spell\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"/home/connectwithprakash/Downloads/slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    #return(' '.join(user_string))\n",
    "    return(' '.join(map(spell,user_string)))\n",
    "import time\n",
    "def clean_data(df):\n",
    "    df=df.str.replace('@\\S+\\s','')\n",
    "    df=df.str.replace('\\s\\S+www\\S+','')\n",
    "    df = df.str.replace(r'\\.+', \" \")\n",
    "    \n",
    "    for ii in range(df.count()):\n",
    "        df[ii] = translator(df[ii])\n",
    "        if ii%100==0:\n",
    "            print(\"Done : {}\".format(ii))\n",
    "        \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define skip-thoughts vectorizer class for scikit-learn\n",
    "class SkipThoughtsVectorizer(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = skipthoughts.load_model()\n",
    "        self.encoder = skipthoughts.Encoder(self.model)\n",
    "\n",
    "    def fit_transform(self, raw_documents, y):\n",
    "        return self.encoder.encode(raw_documents, verbose=False)\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        self.fit_transform(raw_documents, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        return self.fit_transform(raw_documents, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data extraction and cleaning\n",
    "head = ['Emotions', 'Text']\n",
    "df_train = pd.read_csv('train_data.csv')\n",
    "df_train.columns = head\n",
    "df_train = df_train.replace('empty',np.NaN).dropna().reset_index(drop=True)\n",
    "\n",
    "X = df_train['Text']\n",
    "y = df_train['Emotions']\n",
    "\n",
    "X= clean_data(X)\n",
    "\n",
    "#Pickling data for future evaluation\n",
    "pickling_on = open(\"./EmotionAnalysis_data_X_train.pickle\",\"wb\")\n",
    "pickle.dump(X,pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(\"./EmotionAnalysis_data_Y.pickle_train\",\"wb\")\n",
    "pickle.dump(y,pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickling_on = open(\"./EmotionAnalysis_data_X_train.pickle\",\"rb\")\n",
    "X_pickle = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(\"./EmotionAnalysis_data_Y_train.pickle\",\"rb\")\n",
    "y_pickle = pickle.load(pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pickle, y_pickle, random_state=random_seed, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Accuracy : 0.9910105657805044\n",
      "Test set Accuracy     : 0.34026239563809846\n"
     ]
    }
   ],
   "source": [
    "#funcion call to get best estimator for finding best param from grid search\n",
    "best_estimator = LinearSVC()\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#pipeline handle to get work done in pipeline manner which first vectorizes all the text and then uses classification on them\n",
    "model_pipeline = Pipeline([('tfidf', tfidf ),\n",
    "                          ('estimator', best_estimator)])\n",
    "\n",
    "\n",
    "#pipeline_skipthought = Pipeline(steps=[('vectorizer', SkipThoughtsVectorizer()),\n",
    "#                        ('classifier', LogisticRegression())])\n",
    "\n",
    "#feature_union = ('feature_union', FeatureUnion([\n",
    "#    ('skipthought', SkipThoughtsVectorizer()),\n",
    "#    ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),]))\n",
    "#pipeline_both = Pipeline(steps=[feature_union,\n",
    "#                        ('classifier', LogisticRegression())])\n",
    "model_pipeline.fit(X_train,y_train)\n",
    "\n",
    "model=None\n",
    "#final pipelined model for job to do\n",
    "model = model_pipeline\n",
    "\n",
    "pickling_on = open(\"./EmotionAnalysis_model.pickle\",\"wb\")\n",
    "pickle.dump(model,pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "#training score and test score\n",
    "#print('Training set error : {}'.format(model_grid.best_score_))\n",
    "pred=model.predict(X_train)\n",
    "print('Training set Accuracy : {}'.format(accuracy_score(y_train,pred)))\n",
    "pred=model.predict(X_test)\n",
    "print('Test set Accuracy     : {}'.format(accuracy_score(y_test,pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
